{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import skimage.measure\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# USE GPUs? (add .cuda() to all Variables if gpu=1) \n",
    "gpu = 0\n",
    "\n",
    "\n",
    "# GAME SETUP        \n",
    "num_games = 3  ## number of games to play\n",
    "time_steps = 1000  ## max number of time steps per game\n",
    "record = 0  ## record training or game play\n",
    "render = 1  ## show game in real-time\n",
    "\n",
    "\n",
    "# WHERE TO SAVE FILES\n",
    "path_record = '/Users/davidziganto/Repositories/Deep_Reinforcement_Learning/breakout-training'\n",
    "path_save_cnn = '/Users/davidziganto/Repositories/Deep_Reinforcement_Learning/DL_RL_Atari_breakout_3e_1000t'\n",
    "path_save_score = '/Users/davidziganto/Repositories/Deep_Reinforcement_Learning/score.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(rgb_tensor):\n",
    "    '''\n",
    "    Transforms 3D RGB numpy tensor: crop, convert to 2D grayscale, downsample.\n",
    "    '''\n",
    "    crop = rgb_tensor[30:194,:,:]\n",
    "    grayscale = np.dot(crop[...,:3], [0.2989, 0.5870, 0.1140])  ## using Matlab's formula\n",
    "    downsample = skimage.measure.block_reduce(grayscale, (2,2), np.max)\n",
    "    standardize = (downsample - downsample.mean()) / np.sqrt(downsample.var() + 1e-5)\n",
    "    return standardize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Action:\n",
    "    '''Returns an action value which is an int in range [0,3]'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.time_ = 0\n",
    "        \n",
    "    def update_time(self):\n",
    "        self.time_ += 1\n",
    "        \n",
    "    def action(self):\n",
    "        if self.time_ == 0:\n",
    "            self.action_ = 1  ## start game by firing ball\n",
    "        else:\n",
    "            # take agent-based action every 4 time steps; else push action forward w/out agent computing\n",
    "            if self.time_%4 == 0:\n",
    "                if np.random.binomial(n=1, p=eg.epsilon_, size=1):\n",
    "                    self.action_ = env.action_space.sample()  ## take random action\n",
    "                else:\n",
    "                    self.action_ = cnn(Variable(torch.Tensor(initial_seq).unsqueeze(0).unsqueeze(0))).data.max(1)[1][0]  ## take optimal action according to NN\n",
    "        return self.action_\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    '''Long-term memory of experience tuples to break sequential correlations'''\n",
    "    dq_ = deque(maxlen=int(1e6))\n",
    "\n",
    "    def __init__(self, C):\n",
    "        self.capacity_ = C  ## may not need this since not used anywhere?\n",
    "        \n",
    "    def add_experience(self, experience_tuple):\n",
    "        '''add new experience to experience replay'''\n",
    "        self.dq_.append(experience_tuple)\n",
    "        \n",
    "    def sample(self, capacity=32):\n",
    "        '''sample from experience replay'''\n",
    "        nb_items = len(self.dq_)\n",
    "        if nb_items > capacity:\n",
    "            idx = np.random.choice( nb_items, size=capacity, replace=False)\n",
    "        else:\n",
    "            idx = np.random.choice( nb_items, size=nb_items, replace=False)\n",
    "        return [self.dq_[i] for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EpsilonGenerator():\n",
    "    '''Linear annealing'''\n",
    "    def __init__(self, start, stop, steps):\n",
    "        self.epsilon_ = start\n",
    "        self.stop_ = stop\n",
    "        self.steps_ = steps\n",
    "        self.step_size_ = (self.epsilon_ - stop) / (self.steps_)\n",
    "        self.count_ = 1\n",
    "        \n",
    "    def epsilon_update(self):\n",
    "        '''generate next epsilon value'''\n",
    "        if (self.epsilon_ >= self.stop_ and self.count_ < self.steps_):\n",
    "            self.count_ += 1\n",
    "            self.epsilon_ -= self.step_size_\n",
    "        else:\n",
    "            self.epsilon_ = self.stop_\n",
    "            self.count_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    '''Convolutional Neural Network'''\n",
    "    def __init__(self,):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 8, 4)  ## Conv2d(nChannels, filters, kernel, stride)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 4, 4)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 256)\n",
    "        self.fc2 = nn.Linear(256, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 32 * 4 * 4)  ## reshape \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    '''Creates dataset and targets'''    \n",
    "    def __init__(self):\n",
    "        self.replay_size_ = None\n",
    "        self.data_ = None\n",
    "        self.target_ = None\n",
    "        \n",
    "    def get_data(self):\n",
    "        '''get minibatch of dataset and targets'''\n",
    "        self.replay_size_ = len(minibatch)\n",
    "        # create tensor of initial observations\n",
    "        self.data_ = Variable(torch.Tensor([minibatch[i][0] for i in range(self.replay_size_)]).unsqueeze(1))\n",
    "        # create tensor of corresponding target variable values\n",
    "        target_list = []\n",
    "        for i in range(self.replay_size_):\n",
    "            observed = Variable(torch.Tensor(minibatch[i][3]))\n",
    "            if minibatch[i][4] == 'terminal':\n",
    "                target_list.append(minibatch[i][2])\n",
    "            else:\n",
    "                target_list.append(minibatch[i][2] + discount * \n",
    "                                   cnn(observed.unsqueeze(0).unsqueeze(0)).data.max(1)[1][0])\n",
    "        self.target_ = Variable(torch.Tensor(target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Score:\n",
    "    '''Tracks score for each game and adds to list'''    \n",
    "    def __init__(self):\n",
    "        self.all_scores_ = []\n",
    "        self.game_score_ = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        '''reset score to zero upon game termination'''\n",
    "        self.game_score_ = 0\n",
    "        \n",
    "    def increment(self, reward):\n",
    "        '''increment score per iteration'''\n",
    "        self.game_score_ += reward\n",
    "        \n",
    "    def update(self):\n",
    "        '''append game score to running list'''\n",
    "        self.all_scores_.append(self.game_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add'l Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-08 10:56:29,671] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "# Atari emulator\n",
    "env = gym.make('Breakout-v0')\n",
    "# whether to record training\n",
    "if record:\n",
    "    env = wrappers.Monitor(env, \n",
    "                           directory=path_record, \n",
    "                           video_callable=None, ## takes video when episode number is perfect cube\n",
    "                           force=True)\n",
    "\n",
    "\n",
    "# INSTANTIATE KEY CLASSES\n",
    "cnn = torch.nn.DataParallel(CNN()).cuda() if gpu else CNN()\n",
    "er = ExperienceReplay(C=1e6)\n",
    "eg = EpsilonGenerator(start=1, stop=0.1, steps=1e6)\n",
    "agent = Action()\n",
    "dataset = Dataset()\n",
    "score = Score()\n",
    "\n",
    "\n",
    "# SETUP VARIABLES\n",
    "discount = 0.9  \n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "# CNN SETUP\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(cnn.parameters(), \n",
    "                          lr=learning_rate, \n",
    "                          alpha=0.99, \n",
    "                          eps=1e-08, \n",
    "                          weight_decay=0, \n",
    "                          momentum=0, \n",
    "                          centered=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished episode 0 of 2\n",
      "finished episode 1 of 2\n",
      "finished episode 2 of 2\n"
     ]
    }
   ],
   "source": [
    "for episode in range(num_games):\n",
    "    \n",
    "    ## start/reset environment + store observation\n",
    "    initial_seq = preprocess(env.reset())\n",
    "    \n",
    "    ## reset score\n",
    "    score.reset()\n",
    "    \n",
    "    for t in range(time_steps):\n",
    "        \n",
    "        ## show game in real-time\n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "        # take action (0=do nothing; 1=fire ball; 2=move right; 3=move left)\n",
    "        action = agent.action()\n",
    "        agent.update_time()\n",
    "        \n",
    "        # update epsilon for epsilon-greedy implementation\n",
    "        eg.epsilon_update()\n",
    "        \n",
    "        # get feedback from emulator\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        # update score\n",
    "        score.increment(reward)\n",
    "        \n",
    "        # preprocess new observation post action    \n",
    "        final_seq = preprocess(observation)\n",
    "        \n",
    "        # stop if no more moves, else continue and update\n",
    "        if done:\n",
    "            er.add_experience((initial_seq, action, reward, final_seq, 'terminal'))\n",
    "            score.update()\n",
    "            break\n",
    "        else:\n",
    "            er.add_experience((initial_seq, action, reward, final_seq, 'nonterminal'))\n",
    "            \n",
    "        # get mini-batch sample from experience replay (fyi - randomizes index)\n",
    "        minibatch = er.sample()\n",
    "        \n",
    "        # get data for updating policy network\n",
    "        dataset.get_data()\n",
    "        \n",
    "        # Update CNN\n",
    "        optimizer.zero_grad()  ## zero the parameter gradients\n",
    "        outputs = cnn(dataset.data_).max(1)[0]  ## feedforward pass\n",
    "        loss = criterion(outputs, dataset.target_)  ## calculate loss\n",
    "        loss.backward()  ## backpropagation\n",
    "        optimizer.step()  ## update network weights\n",
    "            \n",
    "        # set new observation as initial observation\n",
    "        initial_seq = final_seq\n",
    "        \n",
    "    # progress\n",
    "    print(\"finished episode %d of %d\" % (episode, num_games-1)) \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model & Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save CNN\n",
    "torch.save(cnn.state_dict(), path_save_cnn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save List of Scores\n",
    "pickle.dump(score.all_scores_, open( path_save_score, \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
